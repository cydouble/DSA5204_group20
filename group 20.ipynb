{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport os, re\nimport nltk\nfrom collections import Counter\nimport pickle\nimport torch.nn as nn\nimport matplotlib.image as mpimg","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:22.272552Z","iopub.execute_input":"2022-04-29T02:14:22.273117Z","iopub.status.idle":"2022-04-29T02:14:25.624739Z","shell.execute_reply.started":"2022-04-29T02:14:22.273021Z","shell.execute_reply":"2022-04-29T02:14:25.624024Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# load data\nprojections = pd.read_csv('../input/chest-xrays-indiana-university/indiana_projections.csv')\nreports = pd.read_csv('../input/chest-xrays-indiana-university/indiana_reports.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:28.195235Z","iopub.execute_input":"2022-04-29T02:14:28.197403Z","iopub.status.idle":"2022-04-29T02:14:28.278856Z","shell.execute_reply.started":"2022-04-29T02:14:28.197369Z","shell.execute_reply":"2022-04-29T02:14:28.278189Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tag_encoder = {} \nindex_tag = {}\ni = 0\nfor index, row in reports.iterrows():\n    tags = row['Problems'].split(';')\n    for tag in tags:\n        tag = tag.lower().strip()\n        if tag not in tag_encoder:\n            tag_encoder[tag] = i\n            index_tag[i] = tag\n            i += 1  \nonehot_tags = []\n\n# transform to one-hot\nfor index, row in reports.iterrows():\n    curr = [\"0\"] * len(tag_encoder)\n    tags = row['Problems'].split(';')\n    for tag in tags:\n        tag = tag.lower().strip()\n        curr[tag_encoder[tag]] = \"1\"\n    onehot_tags.append(\"\".join(curr))\ncontext = []\nfor index, row in reports.iterrows():\n    context.append(str(row['impression']) + \" \" + str(row['findings']))\nreports['context'] = context # report part\nreports['tag'] = onehot_tags\ndata = projections.set_index('uid').join(reports.set_index('uid'))\nfilename = list(data['filename'])\ncontext = list(data['context'])\ntags = list(data['tag'])\noutput = pd.DataFrame({'filename_1': filename, 'context':context, 'tags':tags})\ntrain = output.loc[:5999]\ntest = output.loc[6000:3499]\ntrain.to_csv('./train.tsv', sep=\"\\t\", index=False, header=None)\ntest.to_csv('./test.tsv', sep=\"\\t\", index=False, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:30.824457Z","iopub.execute_input":"2022-04-29T02:14:30.825008Z","iopub.status.idle":"2022-04-29T02:14:31.444881Z","shell.execute_reply.started":"2022-04-29T02:14:30.824969Z","shell.execute_reply":"2022-04-29T02:14:31.444140Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_2 = pd.merge(data, data, how= 'inner',left_index=True,right_index=True)\ndata_2 = data_2[['filename_x', 'projection_x', 'filename_y', 'projection_y', 'context_x', 'tag_x']]\ndata_2 = data_2[data_2['projection_x']=='Frontal']\ndata_2 = data_2[data_2['projection_y']=='Lateral']\nfilename1 = list(data_2['filename_x'])\nfilename2 = list(data_2['filename_y'])\ncontext = list(data_2['context_x'])\ntags = list(data_2['tag_x'])\noutput_2 = pd.DataFrame({'filename_1': filename1, 'filename_2': filename2, 'context':context, 'tags':tags})\ntrain_2 = output_2.loc[:2799] \ntest_2 = output_2.loc[2800:] # 测试集\ntrain_2.to_csv('./train_2.tsv', sep=\"\\t\", index=False, header=None)\ntest_2.to_csv('./test_2.tsv', sep=\"\\t\", index=False, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:35.243863Z","iopub.execute_input":"2022-04-29T02:14:35.244420Z","iopub.status.idle":"2022-04-29T02:14:35.311817Z","shell.execute_reply.started":"2022-04-29T02:14:35.244381Z","shell.execute_reply":"2022-04-29T02:14:35.311143Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self):\n        self.word2idx = {} \n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)\n\n\ndef build_vocab(captions, threshold):\n    counter = Counter()\n    for i in range(len(captions)):\n        caption = captions[i]\n        for j in range(len(caption)):\n            tokens = nltk.tokenize.word_tokenize(caption[j].lower()) # tokenize\n            counter.update(tokens)\n\n        if (i+1) % 1000 == 0:\n            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(captions)))\n            \n    words = [word for word, cnt in counter.items() if cnt >= threshold] # least frequence\n\n    vocab = Vocabulary()\n    \n    # some special signals\n    vocab.add_word('<pad>')\n    vocab.add_word('<start>')\n    vocab.add_word('<end>')\n    vocab.add_word('<unk>')\n\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    \n    return vocab\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:41.341573Z","iopub.execute_input":"2022-04-29T02:14:41.341860Z","iopub.status.idle":"2022-04-29T02:14:41.353459Z","shell.execute_reply.started":"2022-04-29T02:14:41.341824Z","shell.execute_reply":"2022-04-29T02:14:41.352763Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_captions(filepath):\n    \"\"\"clean and split\"\"\"\n\n    # 去除标点\n    bioclean = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{},0-9]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\",'').strip().lower()).split()\n\n    captions = []\n\n    with open(filepath, \"r\") as file:\n        for line in file:\n            line = line.replace(\"\\n\", \"\").split(\"\\t\")\n            \n            sentence_tokens = []\n            \n            for sentence in line[-2].split(\".\"): \n                tokens = bioclean(sentence)\n                if len(tokens) == 0:\n                    continue\n                caption = \" \".join(tokens)\n                sentence_tokens.append(caption)\n            \n            captions.append(sentence_tokens)\n    \n    return captions \n\nclass iuxray(Dataset):\n    \"\"\"\n    prepare for training and test data\n    \"\"\"\n    def __init__(self, root_dir, tsv_path, image_path, vocab = None, transform=None, k=10):\n        self.root_dir = root_dir \n        self.tsv_path = tsv_path\n        self.image_path = image_path\n        \n        tsv_file = os.path.join(self.root_dir, self.tsv_path)\n        \n        self.captions = create_captions(tsv_file)\n        if vocab is None:\n            self.vocab = build_vocab(self.captions, 3)\n        else:\n            self.vocab = vocab\n        self.data_file = pd.read_csv(tsv_file, delimiter='\\t',encoding='utf-8', header=None)\n        self.transform = transform \n        self.k = k # top k tags\n\n    def __len__(self):\n        return len(self.data_file)\n\n    def __getitem__(self, idx):\n        if self.data_file.shape[1] == 3:\n            img_name = os.path.join(self.root_dir, self.image_path, self.data_file.iloc[idx, 0])\n            image = Image.open(img_name) \n\n            if self.transform:\n                image = self.transform(image)\n\n            caption = self.captions[idx] \n            sentences = []\n\n            # add start and end signal\n            for i in range(min(self.k,len(caption))):\n                tokens = nltk.tokenize.word_tokenize(str(caption[i]).lower())\n                sentence = []\n                sentence.append(self.vocab('<start>'))\n                sentence.extend([self.vocab(token) for token in tokens])\n                sentence.append(self.vocab('<end>'))\n                sentences.append(sentence)\n            \n            # corresponds to the length of the longest sentence in the report\n            max_sent_len = max([len(sentences[i]) for i in range(len(sentences))])\n            \n            # Complete to the longest sentence length\n            for i in range(len(sentences)):\n                if len(sentences[i]) < max_sent_len:\n                    sentences[i] = sentences[i] + (max_sent_len - len(sentences[i]))* [self.vocab('<pad>')]\n\n            target = torch.Tensor(sentences) \n            tag = self.data_file.iloc[idx, 2] # tag的one hot\n            return image, target, tag, self.k, max_sent_len # len_sentences = k\n        \n        elif self.data_file.shape[1] == 4:\n            img_name1 = os.path.join(self.root_dir, self.image_path, self.data_file.iloc[idx, 0])\n            img_name2 = os.path.join(self.root_dir, self.image_path, self.data_file.iloc[idx, 1])\n            image1 = Image.open(img_name1) # image data\n            image2 = Image.open(img_name2) \n            \n            if self.transform:\n                image1 = self.transform(image1)\n                image2 = self.transform(image2)\n\n            caption = self.captions[idx] \n            sentences = []\n\n            # add start and end signal\n            for i in range(min(self.k,len(caption))):\n                tokens = nltk.tokenize.word_tokenize(str(caption[i]).lower())\n                sentence = []\n                sentence.append(self.vocab('<start>'))\n                sentence.extend([self.vocab(token) for token in tokens])\n                sentence.append(self.vocab('<end>'))\n                sentences.append(sentence)\n\n            # corresponds to the length of the longest sentence in the report\n            max_sent_len = max([len(sentences[i]) for i in range(len(sentences))])\n\n            # Complete to the longest sentence length\n            for i in range(len(sentences)):\n                if len(sentences[i]) < max_sent_len:\n                    sentences[i] = sentences[i] + (max_sent_len - len(sentences[i]))* [self.vocab('<pad>')]\n\n            target = torch.Tensor(sentences) \n            tag = self.data_file.iloc[idx, -1] # tag的one hot\n            return image1, image2, target, tag, self.k, max_sent_len # len_sentences = k\n            \n\n\ndef collate_fn(data):\n    \"\"\"\n    自定义batch\n    \"\"\"\n    \n    if len(list(zip(*data))) == 5:\n        images, captions, tags, len_sentences, max_sent_len = zip(*data)\n        images = torch.stack(images, 0) \n\n        targets = torch.zeros(len(captions), max(len_sentences), max(max_sent_len)).long() # (batch_size, k, max_sent_len)\n        prob = torch.zeros(len(captions), max(len_sentences)).long() # (batch_size, k)\n        for i, cap in enumerate(captions):\n            for j, sent in enumerate(cap):\n                targets[i, j, :len(sent)] = sent[:] # report\n                prob[i, j] = 1 # real sentence generate prob \n        tag = torch.zeros(len(tags), len(tags[0])).long()\n        for i in range(len(tags)):\n            for j in range(len(tags[0])):\n                tag[i, j] = int(tags[i][j])\n        return images, tag, targets, prob\n    else:\n        images1, images2, captions, tags, len_sentences, max_sent_len = zip(*data)\n        images1 = torch.stack(images1, 0) \n        images2 = torch.stack(images2, 0)\n        targets = torch.zeros(len(captions), max(len_sentences), max(max_sent_len)).long() # (batch_size, k, max_sent_len)\n        prob = torch.zeros(len(captions), max(len_sentences)).long() # (batch_size, k)\n        for i, cap in enumerate(captions):\n            for j, sent in enumerate(cap):\n                targets[i, j, :len(sent)] = sent[:] # report\n                prob[i, j] = 1 # real sentence generate prob\n        tag = torch.zeros(len(tags), len(tags[0])).long()\n        for i in range(len(tags)):\n            for j in range(len(tags[0])):\n                tag[i, j] = int(tags[i][j])\n        return images1, images2, tag, targets, prob\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:45.539409Z","iopub.execute_input":"2022-04-29T02:14:45.540002Z","iopub.status.idle":"2022-04-29T02:14:45.597058Z","shell.execute_reply.started":"2022-04-29T02:14:45.539961Z","shell.execute_reply":"2022-04-29T02:14:45.596253Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_loader(root_dir, tsv_path, image_path, transform, batch_size, shuffle, num_workers, vocab = None):\n    \"\"\"return torch.utils.data.DataLoader\"\"\"\n    data = iuxray(root_dir = root_dir, \n             tsv_path = tsv_path, \n             image_path = image_path,\n             vocab = vocab,\n             transform = transform)\n    \n    data_loader = torch.utils.data.DataLoader(dataset=data, \n                                              batch_size=batch_size,\n                                              shuffle=shuffle,\n                                              num_workers=num_workers,\n                                              collate_fn=collate_fn)\n\n    return data_loader, data.vocab","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:50.684854Z","iopub.execute_input":"2022-04-29T02:14:50.685686Z","iopub.status.idle":"2022-04-29T02:14:50.691492Z","shell.execute_reply.started":"2022-04-29T02:14:50.685649Z","shell.execute_reply":"2022-04-29T02:14:50.690459Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n# CNN encoder\nclass EncoderCNN(nn.Module):\n    def __init__(self, model_name='resnet50', pretrained=False): \n        super(EncoderCNN, self).__init__()\n        self.model_name = model_name\n        self.pretrained = pretrained\n        self.model, self.out_features, self.avg_func, self.bn, self.linear = self.__get_model()\n        self.activation = nn.ReLU()\n\n    def __get_model(self):\n        model = None\n        out_features = None\n        func = None\n        first_layer = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # our images are 1 channel\n        modules = [first_layer]\n        if self.model_name == 'resnet50':\n            resnet = models.resnet50(pretrained=self.pretrained)\n            modules.extend(list(resnet.children())[1:-2])\n            model = nn.Sequential(*modules)\n            out_features = resnet.fc.in_features\n            func = torch.nn.AvgPool2d(kernel_size=14, stride=1)\n        elif self.model_name == 'densenet201':\n            densenet = models.densenet201(pretrained=self.pretrained)\n            modules.extend(list(densenet.features[1:]))\n            model = nn.Sequential(*modules)\n            func = torch.nn.AvgPool2d(kernel_size=14, stride=1)\n            out_features = densenet.classifier.in_features\n        elif self.model_name == 'vgg19':\n            vgg = models.vgg19(pretrained=self.pretrained)\n            modules.extend(list(vgg.features.children())[1:-2])\n            model = nn.Sequential(*modules)\n            func = torch.nn.AvgPool2d(kernel_size=14, stride=1)\n            out_features = list(vgg.features.children())[-3].weight.shape[0]\n        linear = nn.Linear(in_features=out_features, out_features=out_features)\n        bn = nn.BatchNorm1d(num_features=out_features, momentum=0.1)\n        return model, out_features, func, bn, linear\n\n    def forward(self, images):\n        visual_features = self.model(images)\n        avg_features = self.avg_func(visual_features).squeeze()\n        return visual_features, avg_features\n\n# MLC for tag，input: visual features, output: prob of each tags and top k tags\nclass MLC(nn.Module):\n    def __init__(self, classes=119, sementic_features_dim=512, fc_in_features=2048, k=10):\n        super(MLC, self).__init__()\n        self.classifier = nn.Linear(in_features=fc_in_features, out_features=classes)\n        self.embed = nn.Embedding(classes, sementic_features_dim)\n        self.k = k\n        self.softmax = nn.Softmax(dim=-1)\n        self.sem_dim = sementic_features_dim\n        self.__init_weight()\n\n    def __init_weight(self):\n        self.classifier.weight.data.uniform_(-0.1, 0.1)\n        self.classifier.bias.data.fill_(0)\n\n    def forward(self, avg_features):\n        tags = self.softmax(self.classifier(avg_features))\n        semantic_features = self.embed(torch.topk(tags, self.k)[1])\n        return tags, semantic_features","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:53.537798Z","iopub.execute_input":"2022-04-29T02:14:53.538127Z","iopub.status.idle":"2022-04-29T02:14:53.556786Z","shell.execute_reply.started":"2022-04-29T02:14:53.538093Z","shell.execute_reply":"2022-04-29T02:14:53.555975Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class AttentionVisual(nn.Module):\n    def __init__(self, vis_enc_dim, sent_hidden_dim, att_dim):\n        super(AttentionVisual, self).__init__()\n\n        self.enc_att = nn.Linear(vis_enc_dim, att_dim) # W_v\n        self.dec_att = nn.Linear(sent_hidden_dim, att_dim) # W_v,h\n        self.tanh = nn.Tanh()\n        self.full_att = nn.Linear(att_dim, 1) # W_vatt\n        self.softmax = nn.Softmax(dim = 1)\n\n    def forward(self, vis_enc_output, dec_hidden_state):\n        vis_enc_output = vis_enc_output.permute(0, 2, 3, 1)\n        vis_enc_output = vis_enc_output.view(vis_enc_output.size(0), -1, vis_enc_output.size(-1)) # (batch_size, 14*14, 512)\n        vis_enc_att = self.enc_att(vis_enc_output) # (batch_size, num_pixels, att_dim)\n        dec_output = self.dec_att(dec_hidden_state) # (batch_size, att_dim)\n\n        join_output = self.tanh(vis_enc_att + dec_output.unsqueeze(1)) # (batch_size, num_pixels, att_dim)\n\n        join_output = self.full_att(join_output).squeeze(2) # (batch_size, num_pixels)\n\n        att_scores = self.softmax(join_output) # (batch_size, num_pixels)\n\n        att_output = torch.sum(att_scores.unsqueeze(2) * vis_enc_output, dim = 1) # a_att\n\n        return att_output, att_scores\n\nclass AttentionSemantic(nn.Module):\n    def __init__(self, sem_enc_dim, sent_hidden_dim, att_dim):\n        super(AttentionSemantic, self).__init__()\n\n        self.enc_att = nn.Linear(sem_enc_dim, att_dim) # W_a\n        self.dec_att = nn.Linear(sent_hidden_dim, att_dim) # W_a,h\n        self.tanh = nn.Tanh()\n        self.full_att = nn.Linear(att_dim, 1) # W_att\n        self.softmax = nn.Softmax(dim = 1)\n\n    def forward(self, sem_enc_output, dec_hidden_state):\n        sem_enc_output = self.enc_att(sem_enc_output) # (batch_size, no_of_tags, att_dim)\n        dec_output = self.dec_att(dec_hidden_state) # (batch_size, att_dim)\n\n        join_output = self.tanh(sem_enc_output + dec_output.unsqueeze(1)) # (batch_size, no_of_tags, att_dim)\n\n        join_output = self.full_att(join_output).squeeze(2) # (batch_size, no_of_tags)\n\n        att_scores = self.softmax(join_output) # (batch_size, no_of_tags)\n\n        att_output = torch.sum(att_scores.unsqueeze(2) * sem_enc_output, dim = 1) # v_att\n\n        return att_output, att_scores\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:56.606405Z","iopub.execute_input":"2022-04-29T02:14:56.606786Z","iopub.status.idle":"2022-04-29T02:14:56.619721Z","shell.execute_reply.started":"2022-04-29T02:14:56.606748Z","shell.execute_reply":"2022-04-29T02:14:56.618556Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class selfAttentionVisual(nn.Module):\n    def __init__(self, vis_enc_dim, sent_hidden_dim, att_dim,version='ver_all_bn',momentum=0.1):\n        super(AttentionVisual, self).__init__()\n        \n        self.version = version\n        \n        self.enc_att = nn.Linear(vis_enc_dim, vis_enc_dim) # W_v\n        self.bn_enc_att = nn.BatchNorm1d(num_features=vis_enc_dim,momentum=momentum)\n        \n        self.dec_att = nn.Linear(sent_hidden_dim, vis_enc_dim) # W_v,h\n        self.bn_dec_att = nn.BatchNorm1d(num_features=vis_enc_dim,momentum=momentum)\n        \n        self.tanh = nn.Tanh()\n        \n        self.full_att = nn.Linear(vis_enc_dim, vis_enc_dim) # W_vatt\n        self.bn_full_att = nn.BatchNorm1d(num_features=vis_enc_dim,momentum=momentum)\n        \n        self.softmax = nn.Softmax()\n\n    def forward(self, vis_enc_output, dec_hidden_state):\n\n        if self.version == 'ver_all_bn':\n            att_scores = self.ver_all_bn(vis_enc_output,dec_hidden_state)\n            #att_output = torch.sum(att_scores.unsqueeze(2) * vis_enc_output, dim = 1) # a_att\n            att_output = torch.mul(vis_enc_output,att_scores)\n        elif self.version == 'ver_no_bn':\n            att_scores = self.ver_no_bn(vis_enc_output,dec_hidden_state)\n            att_output = torch.mul(vis_enc_output,att_scores) # a_att\n\n        return att_output, att_scores\n    \n    def ver_all_bn(self,vis_enc_output,dec_hidden_state):\n        #vis_enc_output = vis_enc_output.permute(0, 2, 3, 1)\n        #vis_enc_output = vis_enc_output.view(vis_enc_output.size(0), -1, vis_enc_output.size(-1)) # (batch_size, 14*14, 512)\n        \n        #print(vis_enc_output.shape)\n        \n        vis_enc_att = self.bn_enc_att(self.enc_att(vis_enc_output)) # (batch_size, num_pixels, att_dim)\n        dec_output = self.bn_dec_att(self.dec_att(dec_hidden_state.squeeze(1))) # (batch_size, att_dim)\n        \n        join_output = self.tanh(vis_enc_att+dec_output)\n        join_output = self.bn_full_att(self.full_att(join_output))\n        \n        vis_alpha = self.softmax(join_output)\n        \n        return vis_alpha\n\n    def ver_no_bn(self,vis_enc_output,dec_hidden_state):\n        #vis_enc_output = vis_enc_output.permute(0, 2, 3, 1)\n        #vis_enc_output = vis_enc_output.view(vis_enc_output.size(0), -1, vis_enc_output.size(-1)) # (batch_size, 14*14, 512)\n        vis_enc_att = self.enc_att(vis_enc_output) # (batch_size, num_pixels, att_dim)\n        dec_output = self.dec_att(dec_hidden_state.squeeze(1)) # (batch_size, att_dim)\n        \n        join_output = self.tanh(vis_enc_att+dec_output)\n        join_output = self.full_att(join_output)\n        \n        vis_alpha = self.softmax(join_output)\n        \n        return vis_alpha\n    \n    \n\nclass selfAttentionSemantic(nn.Module):\n    def __init__(self, sem_enc_dim, sent_hidden_dim, att_dim,version='ver_self',k=10,momentum=0.1):\n        super(AttentionSemantic, self).__init__()\n        \n        self.version = version\n        \n        self.enc_att = nn.Linear(sent_hidden_dim, sent_hidden_dim) # W_a\n        self.bn_enc_att = nn.BatchNorm1d(num_features=k,momentum=momentum)\n        \n        self.dec_att = nn.Linear(sent_hidden_dim, sent_hidden_dim) # W_a,h\n        self.bn_dec_att = nn.BatchNorm1d(num_features=1,momentum=momentum)\n        \n        self.tanh = nn.Tanh()\n        \n        self.full_att = nn.Linear(sent_hidden_dim, sent_hidden_dim) # W_att\n        self.bn_full_att =nn.BatchNorm1d(num_features=k,momentum=momentum)\n        \n        self.softmax = nn.Softmax()\n        \n        self.SelfAttention = Attention(dim=sem_enc_dim,dropout=0.2)\n\n    def forward(self, sem_enc_output, dec_hidden_state):\n        if self.version=='ver_all_bn':\n            att_scores = self.ver_all_bn(sem_enc_output, dec_hidden_state) # (batch_size, no_of_tags)\n            att_output = torch.mul(att_scores, sem_enc_output).sum(1) # sem_att\n        elif self.version=='ver_no_bn':\n            att_scores = self.ver_no_bn(sem_enc_output, dec_hidden_state)\n            att_output = torch.mul(att_scores, sem_enc_output).sum(1)# sem_att\n        elif self.version=='ver_self':\n            att_scores, att_output = self.ver_self(sem_enc_output)\n\n        return att_output, att_scores\n    \n    def ver_all_bn(self, sem_enc_output, dec_hidden_state):\n        \n        sem_enc_output = self.bn_enc_att(self.enc_att(sem_enc_output)) # (batch_size, no_of_tags, att_dim)\n        dec_output = self.bn_dec_att(self.dec_att(dec_hidden_state.unsqueeze(1))) # (batch_size, att_dim)\n\n        join_output = self.tanh(torch.add(sem_enc_output,dec_output))\n        join_output = self.bn_full_att(self.full_att(join_output))\n        sem_alpha = self.softmax(join_output)\n        \n        return sem_alpha\n    \n    def ver_no_bn(self,sem_enc_output,dec_hidden_state):\n        sem_enc_output = self.enc_att(sem_enc_output) # (batch_size, no_of_tags, att_dim)\n        dec_output = self.dec_att(dec_hidden_state.unsqueeze(1)) # (batch_size, att_dim)\n        \n        join_output = self.tanh(sem_enc_output+dec_output)\n        join_output = self.full_att(join_output)\n        sem_alpha = self.softmax(join_output)\n        \n        return sem_alpha\n    \n    def ver_self(self, sem_enc_output):\n        return self.SelfAttention(sem_enc_output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceLSTM(nn.Module):\n    # def __init__(self, vis_embed_dim, sent_hidden_dim, att_dim, sent_input_dim, word_input_dim, int_stop_dim):\n    def __init__(self, vis_embed_dim, sem_embed_dim, sent_hidden_dim, att_dim, sent_input_dim, word_input_dim, int_stop_dim, version=0):\n        super(SentenceLSTM, self).__init__()\n        self.version = version\n        self.vis_att = AttentionVisual(vis_embed_dim, sent_hidden_dim, att_dim)\n        self.sem_att = AttentionSemantic(sem_embed_dim, sent_hidden_dim, att_dim)\n#         self.vis_att = selfAttentionVisual(vis_embed_dim, sent_hidden_dim, att_dim) # for self attention\n#         self.sem_att = selfAttentionSemantic(sem_embed_dim, sent_hidden_dim, att_dim)\n        \n        self.contextLayer = nn.Linear(vis_embed_dim + sem_embed_dim, sent_input_dim) # W_fc\n        self.contextLayer1 = nn.Linear(vis_embed_dim, sent_input_dim) # only visual feature\n        self.lstm = nn.LSTMCell(sent_input_dim, sent_hidden_dim, bias=True) # LSTMCell\n\n        self.sent_hidden_dim = sent_hidden_dim \n        self.word_input_dim = word_input_dim \n\n        self.topic_hid_layer = nn.Linear(sent_hidden_dim, word_input_dim) \n        self.topic_context_layer = nn.Linear(sent_input_dim, word_input_dim)\n        self.tanh1 = nn.Tanh()\n\n        self.stop_prev_hid = nn.Linear(sent_hidden_dim, int_stop_dim)\n        self.stop_cur_hid = nn.Linear(sent_hidden_dim, int_stop_dim)\n        self.tanh2 = nn.Tanh()\n        self.final_stop_layer = nn.Linear(int_stop_dim, 2)\n\n    def forward(self, vis_enc_output, tags, device):\n        if self.version == 0:\n            return self.v0(vis_enc_output, tags, device)\n        elif self.version == 1:\n            return self.v1(vis_enc_output, tags, device)\n    \n    def v0(self, vis_enc_output, tags, device):\n        batch_size = vis_enc_output.shape[0]\n        vis_enc_dim = vis_enc_output.shape[-1]\n        sem_enc_dim = tags.shape[-1]\n\n        sem_enc_ouput = tags.view(batch_size, -1, sem_enc_dim) \n        \n        h = torch.zeros((batch_size, self.sent_hidden_dim)).to(device)\n        c = torch.zeros((batch_size, self.sent_hidden_dim)).to(device)\n        \n        topics = torch.zeros((batch_size, tags.shape[1], self.word_input_dim)).to(device) # topics矩阵 (batch_size, k, word_input_dim)\n        ps = torch.zeros((batch_size, tags.shape[1], 2)).to(device)\n        for t in range(tags.shape[1]):\n            vis_att_output, vis_att_scores = self.vis_att(vis_enc_output, h) # (batch_size, vis_enc_dim), (batch_size, num_pixels)\n            sem_att_output, sem_att_scores = self.sem_att(sem_enc_ouput, h) # \n            \n#             vis_att_output, vis_att_scores = self.vis_att(vis_enc_output, h) # for self attention (batch_size, vis_enc_dim), (batch_size, num_pixels)\n#             sem_att_output, sem_att_scores = self.sem_att(tags, h) #\n\n            context_output = self.contextLayer(torch.cat([vis_att_output, sem_att_output], dim = 1)) # (batch_size, sent_input_dim)\n\n            h_prev = h.clone()\n\n            h, c = self.lstm(context_output, (h, c)) # (batch_size, sent_hidden_dim), (batch_size, sent_hidden_dim)\n\n            topic = self.tanh1(self.topic_hid_layer(h) + self.topic_context_layer(context_output)) # (batch_size, word_input_dim)\n\n            p = self.tanh2(self.stop_prev_hid(h_prev) + self.stop_cur_hid(h)) # (batch_size, int_stop_dim)\n            p = self.final_stop_layer(p) # (batch_size, 2)\n            p = torch.softmax(p, 1)\n            topics[:, t, :] = topic\n            ps[:, t, :] = p\n        return topics, ps\n    \n    def v1(self, vis_enc_output, tags, device):\n        # visual feature only\n        batch_size = vis_enc_output.shape[0]\n        vis_enc_dim = vis_enc_output.shape[-1]\n\n        h = torch.zeros((batch_size, self.sent_hidden_dim)).to(device)\n        c = torch.zeros((batch_size, self.sent_hidden_dim)).to(device)\n        \n        topics = torch.zeros((batch_size, tags.shape[1], self.word_input_dim)).to(device) # topics矩阵 (batch_size, k, word_input_dim)\n        ps = torch.zeros((batch_size, tags.shape[1], 2)).to(device)\n\n        for t in range(tags.shape[1]):\n            vis_att_output, vis_att_scores = self.vis_att(vis_enc_output, h) # (batch_size, vis_enc_dim), (batch_size, num_pixels)\n\n            context_output = self.contextLayer1(vis_att_output) # (batch_size, sent_input_dim)\n\n            h_prev = h.clone()\n\n            h, c = self.lstm(context_output, (h, c)) # (batch_size, sent_hidden_dim), (batch_size, sent_hidden_dim)\n\n            topic = self.tanh1(self.topic_hid_layer(h) + self.topic_context_layer(context_output)) # (batch_size, word_input_dim)\n\n            p = self.tanh2(self.stop_prev_hid(h_prev) + self.stop_cur_hid(h)) # (batch_size, int_stop_dim)\n            p = self.final_stop_layer(p) # (batch_size, 2)\n\n            topics[:, t, :] = topic\n            ps[:, t, :] = p\n        return topics, ps\n\n\nclass WordLSTM(nn.Module):\n    def __init__(self, word_input_dim, word_hidden_dim, vocab_size, num_layers = 1):\n        super(WordLSTM, self).__init__()\n        self.word_hidden_dim = word_hidden_dim\n        self.word_input_dim = word_input_dim\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, word_input_dim)\n        self.lstm = nn.LSTMCell(word_input_dim, word_hidden_dim, bias=True)\n        self.fc = nn.Linear(word_hidden_dim, vocab_size)\n\n    def forward(self, topic, caption, device): # teacher forcing training\n        \n        # topic: batch_size, 512\n        # caption: batch_size, max_sent_len\n        # 要生成outputs: batch_size, max_sent_len, vocab_size\n        outputs = torch.zeros((caption.shape[0], caption.shape[1], self.vocab_size)).to(device)\n        embeddings = self.embedding(caption).to(device) # (batch_size, max_sent_len, word_input_dim)\n        h = topic.to(device)\n        c = torch.zeros((caption.shape[0], self.word_hidden_dim)).to(device)\n        for i in range(caption.shape[1]): # 对一句话中的每一个词\n            x = embeddings[:, i, :] # (batch_size, word_input_dim)\n            h, c = self.lstm(x, (h, c))\n            output = self.fc(h) # (batch_size, vocab_size)\n            outputs[:, i, :] = output\n        return outputs[:, :-1, :]\n        \n        \n    \n    def val(self, topic, max_sent_len, device):\n        outputs = torch.zeros((topic.shape[0], max_sent_len, self.vocab_size)).to(device)\n        start = torch.tensor([[1] for _ in range(topic.shape[0])]).to(device)\n        x = self.embedding(start).to(device) # (batch_size, max_sent_len, word_input_dim)\n        h = topic.to(device)\n        c = torch.zeros((topic.shape[0], self.word_hidden_dim)).to(device)\n        for i in range(max_sent_len): # 对一句话中的每一个词\n            x = x[:, 0, :] # (batch_size, word_input_dim)\n            h, c = self.lstm(x, (h, c))\n            output = self.fc(h) # (batch_size, vocab_size)\n            outputs[:, i, :] = output\n            values, indices = torch.topk(output, 1)\n            x = self.embedding(indices).to(device)\n        return outputs\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:14:59.688112Z","iopub.execute_input":"2022-04-29T02:14:59.688422Z","iopub.status.idle":"2022-04-29T02:14:59.720100Z","shell.execute_reply.started":"2022-04-29T02:14:59.688392Z","shell.execute_reply":"2022-04-29T02:14:59.718984Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def script(root_dir, train_tsv_path, val_tsv_path, image_path, img_size=224, crop_size=224, batch_size=8, shuffle=True, num_workers=0,\n            enc_dim=512, sent_hidden_dim=512, att_dim=512, sent_input_dim=512, word_input_dim=512, int_stop_dim=512,\n            word_hidden_dim=512, num_layers=1,\n            learning_rate_cnn=1e-5, learning_rate_word=5e-4, learning_rate_sent=1e-5, weight_decay_cnn=1e-4, weight_decay_sent=1e-4, weight_decay_word=1e-4,\n            num_epochs=200, lambda_sent=1.0, lambda_word=1.0, lambda_mlc=1.0, log_step=100, save_step=50, extend=False):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([ \n        transforms.Resize(img_size),\n        transforms.RandomCrop(crop_size),\n        transforms.RandomHorizontalFlip(), \n        transforms.ToTensor()\n    ])\n\n    train_loader, vocab = get_loader(\n        root_dir, \n        train_tsv_path, \n        image_path, \n        transform, \n        batch_size, \n        shuffle, \n        num_workers)\n\n    vocab_size = len(vocab)\n    print(\"vocab_size: \", vocab_size)\n\n    val_loader, _ = get_loader(\n        root_dir,\n        val_tsv_path,\n        image_path,\n        transform,\n        batch_size,\n        shuffle,\n        num_workers,\n        vocab)\n    \n    encoderCNN = EncoderCNN('vgg19', pretrained=True).to(device)\n    if extend:\n        mlc = MLC(fc_in_features=encoderCNN.out_features*2).to(device)\n    else:\n        mlc = MLC(fc_in_features=encoderCNN.out_features).to(device)\n    sentLSTM = SentenceLSTM(encoderCNN.out_features, mlc.sem_dim, sent_hidden_dim=512, att_dim=512, sent_input_dim=512, word_input_dim=512, int_stop_dim=512).to(device)\n    wordLSTM = WordLSTM(word_input_dim, word_hidden_dim, vocab_size, num_layers).to(device)\n    \n    encoderCNN.load_state_dict(torch.load('../input/models/encoderCNN.ckpt'))\n    mlc.load_state_dict(torch.load('../input/models/mlc.ckpt'))\n    sentLSTM.load_state_dict(torch.load('../input/models/sentLSTM.ckpt'))\n    wordLSTM.load_state_dict(torch.load('../input/models/wordLSTM.ckpt'))\n    \n#     criterion_stop = nn.CrossEntropyLoss().to(device)\n#     criterion_words = nn.CrossEntropyLoss().to(device)\n\n#     params_cnn = list(encoderCNN.parameters()) + list(mlc.parameters())\n#     params_sent = list(sentLSTM.parameters()) \n#     params_word = list(wordLSTM.parameters())\n#     optim_cnn = torch.optim.Adam(params = params_cnn, lr=learning_rate_cnn, weight_decay=weight_decay_cnn)\n#     optim_sent = torch.optim.Adam(params = params_sent, lr=learning_rate_sent, weight_decay=weight_decay_sent)\n#     optim_word = torch.optim.Adam(params = params_word, lr=learning_rate_word, weight_decay=weight_decay_word)\n\n\n#     total_step = len(train_loader)\n    \n    \n#     for epoch in range(num_epochs):\n#         encoderCNN.train()\n#         mlc.train()\n#         sentLSTM.train()\n#         wordLSTM.train()\n#         if not extend:\n#             for i, (images, tags, captions, prob) in enumerate(train_loader):\n#                 optim_cnn.zero_grad()\n#                 optim_word.zero_grad()\n#                 optim_sent.zero_grad()\n#                 batch_size = images.shape[0]\n#                 images = images.to(device)\n#                 captions = captions.to(device)\n#                 prob = prob.to(device)\n#                 tags = tags.to(device)\n\n#                 vis_enc_output, avg_enc_output = encoderCNN(images)\n#                 pred_tags, semantic_features = mlc(avg_enc_output)\n\n#     #             log_prob = torch.nn.functional.log_softmax(pred_tags, dim=1).to(device)\n#     #             loss_func_mse = nn.MSELoss().to(device)\n#                 loss_func_ce = nn.BCELoss().to(device)\n#                 loss_mlc = loss_func_ce(pred_tags, tags.to(torch.float))\n#                 # semantic_features = torch.randn((batch_size, 10, 512))\n#                 topics, ps = sentLSTM(vis_enc_output, semantic_features, device)\n\n#                 loss_word = torch.tensor([0.0]).to(device)\n#                 for j in range(captions.shape[1]): # teacher forcing了\n#                     word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :], device) # (batch_size, max_sent_len, vocab_size)\n#                     loss_word += criterion_words(word_outputs.contiguous().view(-1, vocab_size), captions[:, j, 1:].contiguous().view(-1))\n#                 ps = torch.log(ps)\n#                 loss_sent = nn.NLLLoss()(ps.view(-1, 2), prob.view(-1))\n#                 loss = lambda_sent * loss_sent + lambda_word * loss_word  + lambda_mlc * loss_mlc\n#     #             loss = loss_mlc\n#                 loss.backward()\n#                 optim_cnn.step()\n#                 optim_word.step()\n#                 optim_sent.step()\n\n\n#                 if i % log_step == 0:\n#                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n#                           .format(epoch, num_epochs, i, total_step, loss.item())) \n#                     print('sent_loss:{}, word_loss:{}, tag_loss:{}'\n#                           .format(loss_sent, loss_word.item(), loss_mlc))\n\n\n#             # 保存模型参数\n#             if (epoch+1) % save_step == 0: \n#                 torch.save(encoderCNN.state_dict(), 'encoderCNN'+str(epoch+63)+'.ckpt')\n#                 torch.save(mlc.state_dict(), 'mlc'+str(epoch+63)+'.ckpt')\n#                 torch.save(sentLSTM.state_dict(), 'sentLSTM'+str(epoch+63)+'.ckpt')\n#                 torch.save(wordLSTM.state_dict(), 'wordLSTM'+str(epoch+63)+'.ckpt')\n#         else:\n#             for i, (images1, images2, tags, captions, prob) in enumerate(train_loader):\n#                 optim_cnn.zero_grad()\n#                 optim_word.zero_grad()\n#                 optim_sent.zero_grad()\n                \n#                 batch_size = images1.shape[0]\n#                 images1 = images1.to(device)\n#                 images2 = images2.to(device)\n#                 captions = captions.to(device)\n#                 prob = prob.to(device)\n#                 tags = tags.to(device)\n\n#                 vis_enc_output_1, avg_enc_output_1 = encoderCNN(images1)\n#                 vis_enc_output_2, avg_enc_output_2 = encoderCNN(images2)\n#                 vis_enc_output = torch.cat((vis_enc_output_1, vis_enc_output_2), 2)\n#                 avg_enc_output = torch.cat((avg_enc_output_1, avg_enc_output_2), 1)\n#                 pred_tags, semantic_features = mlc(avg_enc_output)\n\n#     #             log_prob = torch.nn.functional.log_softmax(pred_tags, dim=1).to(device)\n#     #             loss_func_mse = nn.MSELoss().to(device)\n#                 loss_func_ce = nn.BCELoss().to(device)\n#                 loss_mlc = loss_func_ce(pred_tags, tags.to(torch.float))\n#                 # semantic_features = torch.randn((batch_size, 10, 512))\n#                 topics, ps = sentLSTM(vis_enc_output, semantic_features, device)\n\n#                 loss_word = torch.tensor([0.0]).to(device)\n#                 for j in range(captions.shape[1]): # teacher forcing了\n#                     word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :], device) # (batch_size, max_sent_len, vocab_size)\n#                     loss_word += criterion_words(word_outputs.contiguous().view(-1, vocab_size), captions[:, j, 1:].contiguous().view(-1))\n#                 ps = torch.log(ps)\n#                 loss_sent = nn.NLLLoss()(ps.view(-1, 2), prob.view(-1))\n#                 loss = lambda_sent * loss_sent + lambda_word * loss_word  + lambda_mlc * loss_mlc\n#     #             loss = loss_mlc\n#                 loss.backward()\n#                 optim_cnn.step()\n#                 optim_word.step()\n#                 optim_sent.step()\n\n\n#                 if i % log_step == 0:\n#                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n#                           .format(epoch, num_epochs, i, total_step, loss.item())) \n#                     print('sent_loss:{}, word_loss:{}, tag_loss:{}'\n#                           .format(loss_sent, loss_word.item(), loss_mlc))\n\n\n#             # 保存模型参数\n#             if (epoch+1) % save_step == 0: \n#                 torch.save(encoderCNN.state_dict(), 'encoderCNN'+str(epoch)+'_2.ckpt')\n#                 torch.save(mlc.state_dict(), 'mlc'+str(epoch)+'_2.ckpt')\n#                 torch.save(sentLSTM.state_dict(), 'sentLSTM'+str(epoch)+'_2.ckpt')\n#                 torch.save(wordLSTM.state_dict(), 'wordLSTM'+str(epoch)+'_2.ckpt')\n            \n    return evaluate(val_loader, encoderCNN, mlc, sentLSTM, wordLSTM, vocab, device, extend)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:28:28.648068Z","iopub.execute_input":"2022-04-29T02:28:28.648387Z","iopub.status.idle":"2022-04-29T02:28:28.666522Z","shell.execute_reply.started":"2022-04-29T02:28:28.648358Z","shell.execute_reply":"2022-04-29T02:28:28.665254Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def evaluate(val_loader, encoderCNN, mlc, sentLSTM, wordLSTM, vocab, device, extend=False):\n    encoderCNN.eval()\n    sentLSTM.eval()\n    wordLSTM.eval()\n    mlc.eval()\n\n    vocab_size = len(vocab)\n\n#     criterion_stop_val = nn.CrossEntropyLoss().to(device)\n#     criterion_words_val = nn.CrossEntropyLoss().to(device)\n\n    references = list()\n    hypotheses = list()\n    \n    if not extend:\n        for i, (images, tags, captions, prob) in enumerate(val_loader):\n            images = images.to(device)\n            captions = captions.to(device)\n            prob = prob.to(device)\n            tags = tags.to(device)\n\n\n            vis_enc_output, avg_enc_output = encoderCNN(images)\n            pred_tags, semantic_features = mlc(avg_enc_output)\n    #         loss_func_ce = nn.BCELoss().to(device)\n    #         loss_mlc = loss_func_ce(pred_tags, tags.to(torch.float))\n            topics, ps = sentLSTM(vis_enc_output, semantic_features, device)\n\n\n    #         loss_word = torch.tensor([0.0]).to(device)\n            pred_words = torch.zeros((captions.shape[0], captions.shape[1], captions.shape[2]-1)) # (batch_size, sent_num, max_sent_len)\n            for j in range(captions.shape[1]):\n                # word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :], device)\n                word_outputs = wordLSTM.val(topics[:, j, :], captions.shape[2]-1, device)\n    #             loss_word += criterion_words_val(word_outputs.contiguous().view(-1, vocab_size), captions[:, j, 1:].contiguous().view(-1))\n                _, words = torch.max(word_outputs, 2)\n                pred_words[:, j, :] = words\n    #         loss_sent = criterion_stop_val(ps.view(-1, 2), prob.view(-1))\n    #         loss = loss_sent + loss_word # + loss_mlc\n            # ps = nn.Softmax(dim=2)(ps)\n            for j in range(captions.shape[0]):\n                pred_caption = []\n                target_caption = []\n                pred_tag = []\n                target_tag = []\n                for t in range(tags.shape[1]):\n                        if tags[j, t] == 1:\n                            target_tag.append(index_tag[t])\n                        if pred_tags[j, t] >= 0.05:\n                            pred_tag.append(index_tag[t])\n                print(\"real tags: {}\".format(\"; \".join(target_tag)))\n                print(\"pred tags: {}\".format(\"; \".join(pred_tag)))\n                for k in range(captions.shape[1]):\n                    if float(ps[j, k, 1]) > 0.5:\n                        words_x = pred_words[j, k, :].tolist()\n                        cap = \" \".join([vocab.idx2word[w] for w in words_x if w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'], vocab.word2idx['<end>']}]) + \".\"\n                        if cap != \".\":\n                            pred_caption.append(cap)\n\n\n                    if prob[j, k] == 1:\n                        words_y = captions[j, k, :].tolist()\n                        target_caption.append(\" \".join([vocab.idx2word[w] for w in words_y if w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'], vocab.word2idx['<end>']}]) + \".\")\n                print(\"real captions: {}\".format(\" \".join(target_caption)))\n                print(\"pred captions: {}\".format(\" \".join(pred_caption)))\n                hypotheses.append(pred_caption)\n                references.append(target_caption)\n                print()\n    else:\n        for i, (images1, images2, tags, captions, prob) in enumerate(val_loader):\n            images1 = images1.to(device)\n            images2 = images2.to(device)\n            captions = captions.to(device)\n            prob = prob.to(device)\n            tags = tags.to(device)\n\n\n            vis_enc_output_1, avg_enc_output_1 = encoderCNN(images1)\n            vis_enc_output_2, avg_enc_output_2 = encoderCNN(images2)\n            vis_enc_output = torch.cat((vis_enc_output_1, vis_enc_output_2), 2)\n            avg_enc_output = torch.cat((avg_enc_output_1, avg_enc_output_2), 1)\n            pred_tags, semantic_features = mlc(avg_enc_output)\n    #         loss_func_ce = nn.BCELoss().to(device)\n    #         loss_mlc = loss_func_ce(pred_tags, tags.to(torch.float))\n            topics, ps = sentLSTM(vis_enc_output, semantic_features, device)\n\n\n    #         loss_word = torch.tensor([0.0]).to(device)\n            pred_words = torch.zeros((captions.shape[0], captions.shape[1], captions.shape[2]-1)) # (batch_size, sent_num, max_sent_len)\n            for j in range(captions.shape[1]):\n                # word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :], device)\n                word_outputs = wordLSTM.val(topics[:, j, :], captions.shape[2]-1, device)\n    #             loss_word += criterion_words_val(word_outputs.contiguous().view(-1, vocab_size), captions[:, j, 1:].contiguous().view(-1))\n                _, words = torch.max(word_outputs, 2)\n                pred_words[:, j, :] = words\n    #         loss_sent = criterion_stop_val(ps.view(-1, 2), prob.view(-1))\n    #         loss = loss_sent + loss_word # + loss_mlc\n            # ps = nn.Softmax(dim=2)(ps)\n            for j in range(captions.shape[0]):\n                pred_caption = []\n                target_caption = []\n                pred_tag = []\n                target_tag = []\n                for t in range(tags.shape[1]):\n                        if tags[j, t] == 1:\n                            target_tag.append(index_tag[t])\n                        if pred_tags[j, t] >= 0.05:\n                            pred_tag.append(index_tag[t])\n                print(\"real tags: {}\".format(\"; \".join(target_tag)))\n                print(\"pred tags: {}\".format(\"; \".join(pred_tag)))\n                for k in range(captions.shape[1]):\n                    if float(ps[j, k, 1]) > 0.5:\n                        words_x = pred_words[j, k, :].tolist()\n                        cap = \" \".join([vocab.idx2word[w] for w in words_x if w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'], vocab.word2idx['<end>']}]) + \".\"\n                        if cap != \".\":\n                            pred_caption.append(cap)\n\n\n                    if prob[j, k] == 1:\n                        words_y = captions[j, k, :].tolist()\n                        target_caption.append(\" \".join([vocab.idx2word[w] for w in words_y if w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'], vocab.word2idx['<end>']}]) + \".\")\n                print(\"real captions: {}\".format(\" \".join(target_caption)))\n                print(\"pred captions: {}\".format(\" \".join(pred_caption)))\n                hypotheses.append(pred_caption)\n                references.append(target_caption)\n                print()\n        \n\n    assert len(references) == len(hypotheses)\n    return hypotheses, references","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:15:20.469777Z","iopub.execute_input":"2022-04-29T02:15:20.470108Z","iopub.status.idle":"2022-04-29T02:15:20.501652Z","shell.execute_reply.started":"2022-04-29T02:15:20.470073Z","shell.execute_reply":"2022-04-29T02:15:20.500956Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"hypotheses, references = script('', './train.tsv', './test.tsv', '../input/chest-xrays-indiana-university/images/images_normalized', num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:28:33.881769Z","iopub.execute_input":"2022-04-29T02:28:33.882566Z","iopub.status.idle":"2022-04-29T02:29:14.948201Z","shell.execute_reply.started":"2022-04-29T02:28:33.882524Z","shell.execute_reply":"2022-04-29T02:29:14.947485Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.copytree(r'../input/cococaptions/cococaption', r'./cococaption')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:17:38.797393Z","iopub.execute_input":"2022-04-29T02:17:38.797656Z","iopub.status.idle":"2022-04-29T02:17:42.731488Z","shell.execute_reply.started":"2022-04-29T02:17:38.797628Z","shell.execute_reply":"2022-04-29T02:17:42.730775Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from cococaption.pycocotools.coco import COCO\nfrom cococaption.pycocoevalcap.eval import COCOEvalCap\nimport json\n\ndef evalscores(hypotheses, references):\n    targ_annotations = list()\n    res_annotations = list()\n    img_annotations = list()\n    coco_ann_file = 'coco.json'\n    res_ann_file = 'res.json'\n\n    for i in range(len(hypotheses)):\n        targ_anno_dict = {\"image_id\": i,\n                          \"id\": i,\n                          \"caption\": \" \".join(references[i])}\n\n        targ_annotations.append(targ_anno_dict)\n\n        res_anno_dict = {\"image_id\": i,\n                         \"id\": i,\n                         \"caption\": \" \".join(hypotheses[i])}\n\n        res_annotations.append(res_anno_dict)\n\n        image_anno_dict = {\"id\": i,\n                           \"file_name\": i}\n\n        img_annotations.append(image_anno_dict)\n\n    coco_dict = {\"type\": 'captions', \n                 \"images\": img_annotations, \n                 \"annotations\": targ_annotations}\n\n    res_dict = {\"type\": 'captions', \n                \"images\": img_annotations, \n                \"annotations\": res_annotations}\n\n    with open(coco_ann_file, 'w') as fp:\n        json.dump(coco_dict, fp)\n\n    with open(res_ann_file, 'w') as fs:\n        json.dump(res_annotations, fs)\n\n    coco = COCO(coco_ann_file)\n    cocoRes = coco.loadRes(res_ann_file)\n\n    cocoEval = COCOEvalCap(coco, cocoRes)\n\n    cocoEval.evaluate()\n\n    for metric, score in cocoEval.eval.items():\n        print('%s: %.3f'%(metric, score))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:17:45.048811Z","iopub.execute_input":"2022-04-29T02:17:45.049455Z","iopub.status.idle":"2022-04-29T02:17:45.195139Z","shell.execute_reply.started":"2022-04-29T02:17:45.049417Z","shell.execute_reply":"2022-04-29T02:17:45.194479Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"evalscores(hypotheses, references)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:29:27.933405Z","iopub.execute_input":"2022-04-29T02:29:27.933659Z","iopub.status.idle":"2022-04-29T02:29:29.972999Z","shell.execute_reply.started":"2022-04-29T02:29:27.933630Z","shell.execute_reply":"2022-04-29T02:29:29.971496Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def generate(image1, image2, vocab):   \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([ \n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.ToTensor()\n    ])\n    image1 = Image.open(image1)\n    image1 = transform(image1).to(device)\n    image1 = image1.unsqueeze(0)\n    \n    if image2:\n        image2 = Image.open(image2)\n        image2 = transform(image2).to(device)\n        image2 = image2.unsqueeze(0)\n    \n    vocab_size = len(vocab)\n    encoderCNN = EncoderCNN('vgg19').to(device)\n    mlc = MLC(fc_in_features=encoderCNN.out_features).to(device)\n    sentLSTM = SentenceLSTM(encoderCNN.out_features, mlc.sem_dim, sent_hidden_dim=512, att_dim=512, sent_input_dim=512, word_input_dim=512, int_stop_dim=512).to(device)\n    wordLSTM = WordLSTM(word_input_dim=512, word_hidden_dim=512, vocab_size=vocab_size, num_layers=1).to(device)\n    \n    encoderCNN.load_state_dict(torch.load('../input/models/encoderCNN112.ckpt'))\n    mlc.load_state_dict(torch.load('../input/models/mlc112.ckpt'))\n    sentLSTM.load_state_dict(torch.load('../input/models/sentLSTM112.ckpt'))\n    wordLSTM.load_state_dict(torch.load('../input/models/wordLSTM112.ckpt'))\n    \n    vis_enc_output_1, avg_enc_output_1 = encoderCNN(image1)\n#     if not image2:\n    vis_enc_output = vis_enc_output_1\n    avg_enc_output = avg_enc_output_1\n#     else:\n#     vis_enc_output_2, avg_enc_output_2 = encoderCNN(image2)\n#     vis_enc_output = torch.cat((vis_enc_output_1, vis_enc_output_2), 2)\n#     avg_enc_output = torch.cat((avg_enc_output_1, avg_enc_output_2), 0)\n        \n    pred_tags, semantic_features = mlc(avg_enc_output)\n    topics, ps = sentLSTM(vis_enc_output, semantic_features, device)\n    \n    pred_words = []\n    for j in range(topics.shape[1]):\n        if ps[0, j, 1] > 0.5:\n            word_outputs = wordLSTM.val(topics[:, j, :], 30, device)\n            _, words = torch.max(word_outputs, 2)\n            pred_words.append(words)\n        else:\n            break\n    pred_caption = []\n    target_caption = []\n    pred_tag = []\n    target_tag = []\n    for t in range(pred_tags.shape[0]):\n        if pred_tags[t] >= 0.05:\n            pred_tag.append(index_tag[t])\n    print(\"pred tags: {}\".format(\"; \".join(pred_tag)))\n    for k in range(len(pred_words)):\n            words_x = pred_words[k][0, :].tolist()\n            cap = \" \".join([vocab.idx2word[w] for w in words_x if w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'], vocab.word2idx['<end>']}]) + \".\"\n            if cap != \".\":\n                pred_caption.append(cap)\n    print(\"pred captions: {}\".format(\" \".join(pred_caption)))\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:27:29.460837Z","iopub.execute_input":"2022-04-29T02:27:29.461173Z","iopub.status.idle":"2022-04-29T02:27:29.478733Z","shell.execute_reply.started":"2022-04-29T02:27:29.461140Z","shell.execute_reply":"2022-04-29T02:27:29.478071Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(create_captions('./train.tsv'), 3)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:25:09.556746Z","iopub.execute_input":"2022-04-29T02:25:09.557446Z","iopub.status.idle":"2022-04-29T02:25:11.981359Z","shell.execute_reply.started":"2022-04-29T02:25:09.557406Z","shell.execute_reply":"2022-04-29T02:25:11.980600Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"generate(\"../input/chest-xrays-indiana-university/images/images_normalized/1600_IM-0390-1001.dcm.png\", None, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T02:27:35.485885Z","iopub.execute_input":"2022-04-29T02:27:35.486364Z","iopub.status.idle":"2022-04-29T02:27:38.765132Z","shell.execute_reply.started":"2022-04-29T02:27:35.486328Z","shell.execute_reply":"2022-04-29T02:27:38.764352Z"},"trusted":true},"execution_count":43,"outputs":[]}]}